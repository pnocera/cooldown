# config.yaml.example
server:
  host: "localhost"
  port: 8080

rate_limits:
  - domain: "api.github.com"
    requests_per_second: 10
  - domain: "api.twitter.com"
    requests_per_second: 5
  - domain: "*.example.com"
    requests_per_second: 20

# Optional: Default rate limit for unspecified domains
default_rate_limit:
  requests_per_second: 1

# Cerebras AI specific rate limiting configuration
cerebras_limits:
  rate_limits:
    use_headers: true           # Enable header-based rate limiting
    header_fallback: true       # Fall back to static limits if headers fail
    header_timeout: 5s          # Max time to wait for fresh header data
    reset_buffer: 100ms         # Buffer time before reset to account for clock skew
  rpm_limit: 1000              # Requests per minute limit
  tpm_limit: 1000000           # Tokens per minute limit
  max_queue_depth: 100         # Maximum number of queued requests
  request_timeout: 10m         # Maximum time a request can wait in queue
  priority_threshold: 0.7      # Usage threshold for priority adjustment (70%)

# Example: Cerebras configuration for production use
# cerebras_limits:
#   rpm_limit: 5000            # Higher limit for production
#   tpm_limit: 5000000         # 5M tokens per minute
#   max_queue_depth: 500       # Larger queue for high traffic
#   request_timeout: 5m        # Shorter timeout for better UX
#   priority_threshold: 0.6    # Lower threshold for more aggressive prioritization

# Example: Cerebras configuration for development/testing
# cerebras_limits:
#   rpm_limit: 60              # 1 request per second
#   tpm_limit: 100000          # 100K tokens per minute
#   max_queue_depth: 20        # Small queue for testing
#   request_timeout: 30s       # Short timeout for quick feedback
#   priority_threshold: 0.8    # Higher threshold for testing behavior

# Model-based routing configuration
model_routing:
  # Enable model-based routing (disabled by default)
  enabled: true
  # Default target URL for requests without a model or with unknown models
  default_target: "https://api.openai.com/v1"
  # Model to endpoint mappings
  models:
    # OpenAI models
    "gpt-4": "https://api.openai.com/v1"
    "gpt-4-turbo": "https://api.openai.com/v1"
    "gpt-3.5-turbo": "https://api.openai.com/v1"
    "gpt-4o": "https://api.openai.com/v1"
    "gpt-4o-mini": "https://api.openai.com/v1"

    # Anthropic models
    "claude-3-opus": "https://api.anthropic.com/v1"
    "claude-3-sonnet": "https://api.anthropic.com/v1"
    "claude-3-haiku": "https://api.anthropic.com/v1"
    "claude-3-5-sonnet": "https://api.anthropic.com/v1"

    # Google models
    "gemini-pro": "https://generativelanguage.googleapis.com/v1beta"
    "gemini-pro-vision": "https://generativelanguage.googleapis.com/v1beta"

    # Cerebras models
    "llama3-8b": "https://api.cerebras.ai/v1"
    "llama3-70b": "https://api.cerebras.ai/v1"
    "mixtral-8x7b": "https://api.cerebras.ai/v1"

    # Other providers
    "zephyr-7b-beta": "https://api.huggingface.co/v1"